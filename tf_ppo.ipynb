{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.distributions as dist\n",
    "import tensorflow.contrib.layers as layers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\USER2~1\\AppData\\Local\\Temp/ipykernel_7256/81311100.py:5: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "tf.set_random_seed(2019)\n",
    "np.random.seed(2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Create Environments</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.multiprocessing_env import SubprocVecEnv\n",
    "\n",
    "num_envs = 16\n",
    "env_name = \"Pendulum-v0\"\n",
    "\n",
    "def make_env():\n",
    "    def _thunk():\n",
    "        env = gym.make(env_name)\n",
    "        return env\n",
    "\n",
    "    return _thunk\n",
    "\n",
    "envs = [make_env() for i in range(num_envs)]\n",
    "envs = SubprocVecEnv(envs)\n",
    "\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Neural Network</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic:\n",
    "    def __init__(self, sess, obs, acs, hidden_size, name, trainable, init_std=1.0):\n",
    "        self.sess = sess\n",
    "        self.obs = obs\n",
    "        self.acs = acs\n",
    "        self.hidden_size = hidden_size\n",
    "        self.name = name\n",
    "        self.trainable = trainable\n",
    "        self.init_std = init_std\n",
    "\n",
    "        self.num_ac = self.acs.get_shape().as_list()[-1]\n",
    "\n",
    "        with tf.variable_scope(name):\n",
    "            self._build_network()\n",
    "\n",
    "    def _build_network(self):\n",
    "        with tf.variable_scope('critic'):\n",
    "            c_h1 = layers.fully_connected(self.obs, self.hidden_size, trainable=self.trainable)\n",
    "            c_out = layers.fully_connected(c_h1, 1, activation_fn=None, trainable=self.trainable)\n",
    "\n",
    "        with tf.variable_scope('actor'):\n",
    "            a_h1 = layers.fully_connected(self.obs, self.hidden_size, trainable=self.trainable)\n",
    "            a_out = layers.fully_connected(a_h1, self.num_ac, activation_fn=None, trainable=self.trainable)\n",
    "\n",
    "            log_std = tf.get_variable('log_std', [1, self.num_ac], dtype=tf.float32,\n",
    "                                      initializer=tf.constant_initializer(self.init_std),\n",
    "                                      trainable=self.trainable)\n",
    "\n",
    "        std = tf.exp(log_std)\n",
    "        a_dist = dist.Normal(a_out, std)\n",
    "        self.log_prob = a_dist.log_prob(self.acs)\n",
    "        self.entropy = tf.reduce_mean(a_dist.entropy())\n",
    "\n",
    "        self.value = tf.identity(c_out)\n",
    "        self.action = a_dist.sample()\n",
    "\n",
    "    def params(self):\n",
    "        return tf.global_variables(self.name).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>GAE</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(next_value, rewards, masks, values, gamma=0.99, tau=0.95):\n",
    "    values = values + [next_value]\n",
    "    gae = 0\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        delta = rewards[step] + gamma * values[step + 1] * masks[step] - values[step]\n",
    "        gae = delta + gamma * tau * masks[step] * gae\n",
    "        returns.insert(0, gae + values[step])\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Proximal Policy Optimization Algorithm</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self, sess, ob_shape, ac_shape, lr, hidden_size, eps=0.2, v_coeff=0.5, ent_coeff=0.01):\n",
    "        self.sess = sess\n",
    "        self.ob_shape = ob_shape\n",
    "        self.ac_shape = ac_shape\n",
    "        self.lr = lr\n",
    "        self.hidden_size = hidden_size\n",
    "        self.eps = eps\n",
    "        self.v_coeff = v_coeff\n",
    "        self.ent_coeff = ent_coeff\n",
    "\n",
    "        self._create_ppo_graph()\n",
    "\n",
    "    def _create_ppo_graph(self):\n",
    "        self.obs = tf.placeholder(dtype=tf.float32, shape=[None] + self.ob_shape, name='observation')\n",
    "        self.acs = tf.placeholder(dtype=tf.float32, shape=[None] + self.ac_shape, name='action')\n",
    "        self.returns = tf.placeholder(dtype=tf.float32, shape=[None, 1])\n",
    "        self.advs = tf.placeholder(dtype=tf.float32, shape=[None, 1])\n",
    "\n",
    "        self.pi = ActorCritic(self.sess, self.obs, self.acs, self.hidden_size, 'new_pi', trainable=True)\n",
    "        self.old_pi = ActorCritic(self.sess, self.obs, self.acs, self.hidden_size, 'old_pi', trainable=False)\n",
    "\n",
    "        self.pi_param = self.pi.params()\n",
    "        self.old_pi_param = self.old_pi.params()\n",
    "\n",
    "        with tf.name_scope('update_old_policy'):\n",
    "            self.oldpi_update = [oldp.assign(p) for p, oldp in zip(self.pi_param, self.old_pi_param)]\n",
    "\n",
    "        with tf.name_scope('loss'):\n",
    "            ratio = tf.exp(self.pi.log_prob - self.old_pi.log_prob)\n",
    "            surr = ratio * self.advs\n",
    "            self.actor_loss = tf.reduce_mean(\n",
    "                tf.minimum(surr, tf.clip_by_value(ratio, 1 - self.eps, 1 + self.eps) * self.advs))\n",
    "            self.critic_loss = tf.reduce_mean(tf.square(self.returns - self.pi.value))\n",
    "\n",
    "            self.loss = (- self.actor_loss - self.ent_coeff * tf.reduce_mean(self.pi.entropy)\n",
    "                         + self.v_coeff * self.critic_loss)\n",
    "\n",
    "            with tf.variable_scope('train_op'):\n",
    "                grads = tf.gradients(self.loss, self.pi_param)\n",
    "                self.grads = list(zip(grads, self.pi_param))\n",
    "                self.train_op = tf.train.AdamOptimizer(self.lr).apply_gradients(self.grads)\n",
    "                                                                                #global_step=self.global_step)\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        return self.sess.run(self.pi.action, feed_dict={self.obs: obs})\n",
    "\n",
    "    def get_value(self, obs):\n",
    "        return self.sess.run(self.pi.value, feed_dict={self.obs: obs})\n",
    "\n",
    "    def assign_old_pi(self):\n",
    "        self.sess.run(self.oldpi_update)\n",
    "\n",
    "    def update(self, obs, acs, returns, advs):\n",
    "        feed_dict = {self.obs: obs,\n",
    "                     self.acs: acs,\n",
    "                     self.returns: returns,\n",
    "                     self.advs: advs\n",
    "                     }\n",
    "\n",
    "        self.sess.run(self.train_op, feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_iter(mini_batch_size, obs, acs, returns, advantage):\n",
    "    batch_size = obs.shape[0]\n",
    "    for _ in range(batch_size // mini_batch_size):\n",
    "        rand_ids = np.random.randint(0, batch_size, mini_batch_size)\n",
    "        yield (obs[rand_ids, :], acs[rand_ids, :],\n",
    "               returns[rand_ids, :], advantage[rand_ids, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Testing Function</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(frame_idx, rewards):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('frame %s. reward: %s' % (frame_idx, rewards[-1]))\n",
    "    plt.plot(rewards)\n",
    "    plt.show()\n",
    "    \n",
    "def test_env(model, vis=False):\n",
    "    ob = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        if vis:\n",
    "            env.render()\n",
    "        ac = model.get_action([ob])[0]\n",
    "        next_ob, reward, done, _ = env.step(ac)\n",
    "        ob = next_ob\n",
    "        total_reward += reward\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Hyperparameters</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "lr = 3e-4\n",
    "num_steps = 20\n",
    "mini_batch_size = 5\n",
    "ppo_epochs = 4\n",
    "threshold_reward = -200\n",
    "\n",
    "max_frames = 15000\n",
    "frame_idx  = 0\n",
    "test_rewards = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Training</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ob_shape = list(envs.observation_space.shape)\n",
    "ac_shape = list(envs.action_space.shape)\n",
    "\n",
    "ob = envs.reset()\n",
    "early_stop = False\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.InteractiveSession(config=config)\n",
    "ppo = PPO(sess, ob_shape, ac_shape, lr, hidden_size)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "while frame_idx < max_frames and not early_stop:\n",
    "\n",
    "    log_probs = []\n",
    "    values = []\n",
    "    obs = []\n",
    "    acs = []\n",
    "    rewards = []\n",
    "    masks = []\n",
    "    entropy = 0\n",
    "\n",
    "    for _ in range(num_steps):\n",
    "\n",
    "        ac = ppo.get_action(ob)\n",
    "        next_ob, reward, done, _ = envs.step(ac)\n",
    "\n",
    "        value = ppo.get_value(ob)\n",
    "        values.append(value)\n",
    "        rewards.append(reward[:, np.newaxis])\n",
    "        masks.append((1-done)[:, np.newaxis])\n",
    "\n",
    "        obs.append(ob)\n",
    "        acs.append(ac)\n",
    "\n",
    "        ob = next_ob\n",
    "        frame_idx += 1\n",
    "\n",
    "        if frame_idx % 1000 == 0:\n",
    "            test_reward = np.mean([test_env(ppo) for _ in range(10)])\n",
    "            test_rewards.append(test_reward)\n",
    "            plot(frame_idx, test_rewards)\n",
    "            if test_reward > threshold_reward: early_stop = True\n",
    "\n",
    "    next_value = ppo.get_value(next_ob)\n",
    "    returns = compute_gae(next_value, rewards, masks, values)\n",
    "\n",
    "    returns = np.concatenate(returns)\n",
    "    values = np.concatenate(values)\n",
    "    obs = np.concatenate(obs)\n",
    "    acs = np.concatenate(acs)\n",
    "    advantages = returns - values\n",
    "\n",
    "    ppo.assign_old_pi()\n",
    "    for _ in range(ppo_epochs):\n",
    "        for ob_batch, ac_batch, return_batch, adv_batch in ppo_iter(mini_batch_size, obs, acs, returns, advantages):\n",
    "            ppo.update(ob_batch, ac_batch, return_batch, adv_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0 reward: -128.1665568319989\n",
      "episode: 1 reward: -134.83206556254473\n",
      "episode: 2 reward: -386.0625747099126\n",
      "episode: 3 reward: -135.76695168787467\n",
      "episode: 4 reward: -2.614806125431447\n",
      "episode: 5 reward: -285.5061693734549\n",
      "episode: 6 reward: -128.14001188736873\n",
      "episode: 7 reward: -129.95654850830167\n",
      "episode: 8 reward: -254.92752438398472\n",
      "episode: 9 reward: -414.8974497820424\n",
      "episode: 10 reward: -603.183591310851\n",
      "episode: 11 reward: -134.45368093962782\n",
      "episode: 12 reward: -2.0873014458142345\n",
      "episode: 13 reward: -3.430080751360662\n",
      "episode: 14 reward: -247.39486551053307\n",
      "episode: 15 reward: -2.044560049003949\n",
      "episode: 16 reward: -5.4638200696051085\n",
      "episode: 17 reward: -4.36138177791826\n",
      "episode: 18 reward: -261.1144660178172\n",
      "episode: 19 reward: -133.86161776911666\n",
      "episode: 20 reward: -127.45439073018935\n",
      "episode: 21 reward: -135.54655198253369\n",
      "episode: 22 reward: -118.98019387093102\n",
      "episode: 23 reward: -135.22062101618337\n",
      "episode: 24 reward: -251.64087190927592\n",
      "episode: 25 reward: -124.22678114822288\n",
      "episode: 26 reward: -262.3116887104984\n",
      "episode: 27 reward: -134.88799644335703\n",
      "episode: 28 reward: -377.38313459274764\n",
      "episode: 29 reward: -131.11481233845055\n",
      "episode: 30 reward: -3.537455874867134\n",
      "episode: 31 reward: -135.42629064484586\n",
      "episode: 32 reward: -133.4656538723514\n",
      "episode: 33 reward: -128.54703797906993\n",
      "episode: 34 reward: -250.4742263893421\n",
      "episode: 35 reward: -237.04584660625522\n",
      "episode: 36 reward: -128.01136776173655\n",
      "episode: 37 reward: -412.1055292810686\n",
      "episode: 38 reward: -135.638001444024\n",
      "episode: 39 reward: -352.973109252194\n",
      "episode: 40 reward: -132.2378416650885\n",
      "episode: 41 reward: -253.61114989159924\n",
      "episode: 42 reward: -127.90513945719485\n",
      "episode: 43 reward: -127.59965381450509\n",
      "episode: 44 reward: -257.3024415588581\n",
      "episode: 45 reward: -250.28339282364107\n",
      "episode: 46 reward: -247.38909619603453\n",
      "episode: 47 reward: -130.4867773142888\n",
      "episode: 48 reward: -140.21374294465772\n",
      "episode: 49 reward: -133.702450407125\n",
      "episode: 50 reward: -246.29893644813882\n",
      "episode: 51 reward: -256.0564627009842\n",
      "episode: 52 reward: -275.4737381389548\n",
      "episode: 53 reward: -249.8934760674516\n",
      "episode: 54 reward: -368.53891095184093\n",
      "episode: 55 reward: -394.40172804310487\n",
      "episode: 56 reward: -2.321464153601178\n",
      "episode: 57 reward: -251.31819115389038\n",
      "episode: 58 reward: -278.1317663306208\n",
      "episode: 59 reward: -132.5549085808396\n",
      "episode: 60 reward: -128.88201368476766\n",
      "episode: 61 reward: -124.54466948824168\n",
      "episode: 62 reward: -259.68239124396155\n",
      "episode: 63 reward: -270.8099035815743\n",
      "episode: 64 reward: -3.9612158454256585\n",
      "episode: 65 reward: -370.6799168333179\n",
      "episode: 66 reward: -136.09537266282013\n",
      "episode: 67 reward: -133.46747654815593\n",
      "episode: 68 reward: -404.2194690146523\n",
      "episode: 69 reward: -130.81705588043874\n",
      "episode: 70 reward: -2.6759112788394286\n",
      "episode: 71 reward: -135.23322261607768\n",
      "episode: 72 reward: -139.57890016068546\n",
      "episode: 73 reward: -258.37398184797274\n",
      "episode: 74 reward: -357.81465699501445\n",
      "episode: 75 reward: -134.69392035448152\n",
      "episode: 76 reward: -262.4863888639575\n",
      "episode: 77 reward: -127.2954606725654\n",
      "episode: 78 reward: -126.10902570744686\n",
      "episode: 79 reward: -131.8630731144605\n",
      "episode: 80 reward: -244.906198092184\n",
      "episode: 81 reward: -251.19658900091\n",
      "episode: 82 reward: -260.96619746762997\n",
      "episode: 83 reward: -132.10285042370847\n",
      "episode: 84 reward: -244.82575178359377\n",
      "episode: 85 reward: -261.04237158999786\n",
      "episode: 86 reward: -131.1559735639227\n",
      "episode: 87 reward: -258.7695304996133\n",
      "episode: 88 reward: -388.9995784700413\n",
      "episode: 89 reward: -135.0362687388359\n",
      "episode: 90 reward: -133.3797024526858\n",
      "episode: 91 reward: -136.23540454616736\n",
      "episode: 92 reward: -386.6358738296628\n",
      "episode: 93 reward: -2.3606151756602163\n",
      "episode: 94 reward: -399.70499799584235\n",
      "episode: 95 reward: -129.38830167440713\n",
      "episode: 96 reward: -135.89626491146706\n",
      "episode: 97 reward: -134.47596926285155\n",
      "episode: 98 reward: -248.34842214587513\n",
      "episode: 99 reward: -131.21329097607824\n",
      "episode: 100 reward: -268.7499215575537\n",
      "episode: 101 reward: -123.79890925645404\n",
      "episode: 102 reward: -128.87075472693752\n",
      "episode: 103 reward: -134.54671142693175\n",
      "episode: 104 reward: -383.25314550734265\n",
      "episode: 105 reward: -263.91902825775054\n",
      "episode: 106 reward: -252.9896973076681\n",
      "episode: 107 reward: -255.3546277074783\n",
      "episode: 108 reward: -263.9825913662131\n",
      "episode: 109 reward: -130.4697012646491\n",
      "episode: 110 reward: -133.3818027065909\n",
      "episode: 111 reward: -129.55609904604185\n",
      "episode: 112 reward: -252.39633185248962\n",
      "episode: 113 reward: -136.5996107453403\n",
      "episode: 114 reward: -3.8694642265614934\n",
      "episode: 115 reward: -267.4948522745351\n",
      "episode: 116 reward: -140.95037982780877\n",
      "episode: 117 reward: -133.57822088820672\n",
      "episode: 118 reward: -257.9847362548185\n",
      "episode: 119 reward: -3.0056466922129608\n",
      "episode: 120 reward: -2.571026167343288\n",
      "episode: 121 reward: -129.88278402907866\n",
      "episode: 122 reward: -380.32524514634866\n",
      "episode: 123 reward: -130.76024008422516\n",
      "episode: 124 reward: -133.14148113227102\n",
      "episode: 125 reward: -129.02387787216654\n",
      "episode: 126 reward: -2.35317004526195\n",
      "episode: 127 reward: -129.75964042055713\n",
      "episode: 128 reward: -357.6630678347442\n",
      "episode: 129 reward: -251.35625709833738\n",
      "episode: 130 reward: -267.7850503985994\n",
      "episode: 131 reward: -131.11669660965967\n",
      "episode: 132 reward: -129.48110017262448\n",
      "episode: 133 reward: -269.58150420813627\n",
      "episode: 134 reward: -133.97785141109438\n",
      "episode: 135 reward: -379.7499486683601\n",
      "episode: 136 reward: -2.4257090233938428\n",
      "episode: 137 reward: -247.98249299309234\n",
      "episode: 138 reward: -2.6926225824096313\n",
      "episode: 139 reward: -133.67218785731603\n",
      "episode: 140 reward: -272.8219930853319\n",
      "episode: 141 reward: -6.010797051950528\n",
      "episode: 142 reward: -336.5343094606227\n",
      "episode: 143 reward: -250.26160214799665\n",
      "episode: 144 reward: -2.643274110904189\n",
      "episode: 145 reward: -131.12282510700348\n",
      "episode: 146 reward: -254.98265836744923\n",
      "episode: 147 reward: -130.68191603627938\n",
      "episode: 148 reward: -261.73797770451785\n",
      "episode: 149 reward: -133.21114095362938\n",
      "episode: 150 reward: -268.82119013626533\n",
      "episode: 151 reward: -126.19605845065668\n",
      "episode: 152 reward: -134.55719276258503\n",
      "episode: 153 reward: -266.4634345965039\n",
      "episode: 154 reward: -132.21683518075645\n",
      "episode: 155 reward: -131.36585804959887\n",
      "episode: 156 reward: -138.82852210579244\n",
      "episode: 157 reward: -136.78830016342195\n",
      "episode: 158 reward: -137.6393127487576\n",
      "episode: 159 reward: -252.93872718832793\n",
      "episode: 160 reward: -135.98374170770518\n",
      "episode: 161 reward: -2.523230333577622\n",
      "episode: 162 reward: -266.13220484490944\n",
      "episode: 163 reward: -122.2071506384612\n",
      "episode: 164 reward: -128.2158569581688\n",
      "episode: 165 reward: -131.62977626716986\n",
      "episode: 166 reward: -253.76715699717823\n",
      "episode: 167 reward: -458.2180017335964\n",
      "episode: 168 reward: -248.19603837511846\n",
      "episode: 169 reward: -132.74857245648417\n",
      "episode: 170 reward: -3.807840382353104\n",
      "episode: 171 reward: -133.90290880663073\n",
      "episode: 172 reward: -132.47069604812197\n",
      "episode: 173 reward: -242.7073278125229\n",
      "episode: 174 reward: -134.39585261332252\n",
      "episode: 175 reward: -131.29614452695301\n",
      "episode: 176 reward: -128.0703015336145\n",
      "episode: 177 reward: -265.2905269866766\n",
      "episode: 178 reward: -135.84705579887358\n",
      "episode: 179 reward: -127.45228699578574\n",
      "episode: 180 reward: -414.40578148424675\n",
      "episode: 181 reward: -132.07548562811758\n",
      "episode: 182 reward: -131.8360181857919\n",
      "episode: 183 reward: -133.13929481221518\n",
      "episode: 184 reward: -255.6273789807867\n",
      "episode: 185 reward: -2.6279743473408264\n",
      "episode: 186 reward: -252.79687550874246\n",
      "episode: 187 reward: -132.96069036291175\n",
      "episode: 188 reward: -134.17980161506347\n",
      "episode: 189 reward: -263.6456137818477\n",
      "episode: 190 reward: -268.4064326139197\n",
      "episode: 191 reward: -2.7395795503980565\n",
      "episode: 192 reward: -133.48455284612916\n",
      "episode: 193 reward: -124.86993541858115\n",
      "episode: 194 reward: -267.1393266030784\n",
      "episode: 195 reward: -262.5521891366085\n",
      "episode: 196 reward: -134.83564553505528\n",
      "episode: 197 reward: -267.64473809596427\n",
      "episode: 198 reward: -126.36884430051454\n",
      "episode: 199 reward: -138.91723687998405\n",
      "episode: 200 reward: -130.0236255360514\n",
      "episode: 201 reward: -263.9491607395447\n",
      "episode: 202 reward: -257.2776385010803\n",
      "episode: 203 reward: -261.0107160155621\n",
      "episode: 204 reward: -132.49645728082658\n",
      "episode: 205 reward: -404.6000642797781\n",
      "episode: 206 reward: -133.38517076169106\n",
      "episode: 207 reward: -412.69814614866937\n",
      "episode: 208 reward: -135.35096426688582\n",
      "episode: 209 reward: -264.997554888992\n",
      "episode: 210 reward: -132.86914796899518\n",
      "episode: 211 reward: -252.73419340044325\n",
      "episode: 212 reward: -265.9749514303825\n",
      "episode: 213 reward: -132.96520246798764\n",
      "episode: 214 reward: -138.65802470548326\n",
      "episode: 215 reward: -249.8193876870304\n",
      "episode: 216 reward: -141.10565066363964\n",
      "episode: 217 reward: -263.2327135044687\n",
      "episode: 218 reward: -133.62119896689657\n",
      "episode: 219 reward: -133.34317294687781\n",
      "episode: 220 reward: -130.6059213065462\n",
      "episode: 221 reward: -269.21071725409575\n",
      "episode: 222 reward: -127.62448345083892\n",
      "episode: 223 reward: -125.4045360538485\n",
      "episode: 224 reward: -128.32818249543337\n",
      "episode: 225 reward: -130.9964627719951\n",
      "episode: 226 reward: -277.5332170915459\n",
      "episode: 227 reward: -3.375620685794299\n",
      "episode: 228 reward: -410.8187137208611\n",
      "episode: 229 reward: -259.3019401336841\n",
      "episode: 230 reward: -130.11485983182808\n",
      "episode: 231 reward: -3.614312134105475\n",
      "episode: 232 reward: -131.20435061731908\n",
      "episode: 233 reward: -256.71722072949666\n",
      "episode: 234 reward: -135.4013137666205\n",
      "episode: 235 reward: -132.14338054953984\n",
      "episode: 236 reward: -264.95700745746575\n",
      "episode: 237 reward: -246.0250535205459\n",
      "episode: 238 reward: -269.58792161435065\n",
      "episode: 239 reward: -2.8941709202340995\n",
      "episode: 240 reward: -134.02095293059222\n",
      "episode: 241 reward: -265.6028987442419\n",
      "episode: 242 reward: -136.46553944186226\n",
      "episode: 243 reward: -3.8776127691495974\n",
      "episode: 244 reward: -3.6166157162426735\n",
      "episode: 245 reward: -252.76788372920362\n",
      "episode: 246 reward: -129.70355039871512\n",
      "episode: 247 reward: -250.29121422537273\n",
      "episode: 248 reward: -251.22578463142696\n",
      "episode: 249 reward: -247.36728644968716\n",
      "\n",
      "(50000, 4)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from itertools import count\n",
    "\n",
    "max_expert_num = 50000\n",
    "num_steps = 0\n",
    "expert_traj = []\n",
    "\n",
    "for i_episode in count():\n",
    "    ob = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        ac = ppo.get_action([ob])[0]\n",
    "        next_ob, reward, done, _ = env.step(ac)\n",
    "        ob = next_ob\n",
    "        total_reward += reward\n",
    "        expert_traj.append(np.hstack([ob, ac]))\n",
    "        num_steps += 1\n",
    "    \n",
    "    print(\"episode:\", i_episode, \"reward:\", total_reward)\n",
    "    \n",
    "    if num_steps >= max_expert_num:\n",
    "        break\n",
    "        \n",
    "expert_traj = np.stack(expert_traj)\n",
    "print()\n",
    "print(expert_traj.shape)\n",
    "print()\n",
    "np.save(\"expert_traj.npy\", expert_traj)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "18fe45ec89688b38eef2a747055d212f6172ad5c1d55dde33bec25487c7a87db"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('tf_tensorflow': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
